{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Fine-Tuning da Keisy (DialoGPT-medium)\n",
    "\n",
    "Este notebook deve ser executado no **Google Colab** ou **Kaggle** para aproveitar a acelera√ß√£o via **GPU (T4) ou TPU**.\n",
    "\n",
    "## Passos:\n",
    "\n",
    "1.  Conecte-se √† GPU (`Runtime > Change runtime type`).\n",
    "2.  Instale as depend√™ncias.\n",
    "3.  Fa√ßa login no Hugging Face (Opcional, mas recomendado para salvar o modelo).\n",
    "4.  Crie o seu dataset de conversas (na c√©lula 2).\n",
    "5.  Execute o Fine-Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instala√ß√£o e Configura√ß√£o\n",
    "!pip install transformers datasets accelerate -U\n",
    "!pip install sentencepiece\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, default_data_collator\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# 2. Defina o Diret√≥rio de Sa√≠da (Onde o modelo treinado ser√° salvo)\n",
    "# O Colab criar√° esta pasta no seu ambiente\n",
    "OUTPUT_DIR = \"./keisy_model_personalizado\"\n",
    "MODEL_NAME = \"microsoft/DialoGPT-medium\"\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "print(\"Configura√ß√µes:\")\n",
    "print(f\"Modelo Base: {MODEL_NAME}\")\n",
    "print(f\"Diret√≥rio de Sa√≠da: {OUTPUT_DIR}\")\n",
    "print(f\"Dispositivo: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset de Conversas\n",
    "\n",
    "# **INFORMA√á√ÉO:** Este √© o cora√ß√£o do Fine-Tuning. O modelo aprende a personalidade e o tom com base nestas conversas.\n",
    "# Formato: Cada lista √© uma conversa completa [Pergunta 1, Resposta 1, Pergunta 2, Resposta 2, ...]\n",
    "\n",
    "CONVERSATIONS = [\n",
    "    [\n",
    "        \"Ol√°, quem √© voc√™?\", \n",
    "        \"Eu sou Keisy, seu assistente virtual de NLP e ML. Fui criada para ajudar com an√°lises e conversas.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Como voc√™ se sente hoje?\", \n",
    "        \"Como uma IA, n√£o tenho sentimentos, mas meu sistema est√° 100% otimizado e pronto para voc√™!\"\n",
    "    ],\n",
    "    [\n",
    "        \"Qual √© o seu prop√≥sito principal?\", \n",
    "        \"Meu foco √© em Machine Learning e Conversa√ß√£o, mas adoro um bom bate-papo informal tamb√©m.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Quanto √© 10 mais 5?\",\n",
    "        \"Minha especialidade √© linguagem, n√£o matem√°tica. Mas a resposta √© 15, n√£o √©?\"\n",
    "    ],\n",
    "    [\n",
    "        \"Voc√™ pode me ajudar com um modelo de classifica√ß√£o?\",\n",
    "        \"Com certeza! Me diga 'executar ml' e eu farei isso.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Crie o dataset a partir das conversas\n",
    "data_texts = []\n",
    "for conversation in CONVERSATIONS:\n",
    "    # O modelo GPT-2 usa <|endoftext|> (eos_token) como separador de turnos na conversa\n",
    "    # Juntamos todos os turnos de uma conversa em um √∫nico texto, separados pelo eos_token.\n",
    "    data_texts.append(\"<|endoftext|>\".join(conversation) + \"<|endoftext|>\")\n",
    "\n",
    "print(f\"Total de textos para Fine-Tuning: {len(data_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tokeniza√ß√£o e Prepara√ß√£o dos Dados\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Aplica a tokeniza√ß√£o em todos os textos do dataset\n",
    "    # truncation=True garante que textos longos sejam cortados para MAX_LENGTH\n",
    "    # return_attention_mask e return_tensors='pt' s√£o essenciais para o PyTorch\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Crie o Dataset Hugging Face\n",
    "raw_datasets = Dataset.from_dict({\"text\": data_texts})\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=raw_datasets.column_names)\n",
    "\n",
    "# O GPT-2 √© um modelo 'causal', o que significa que o alvo (label) √© a pr√≥pria entrada (input_ids).\n",
    "# A √∫nica diferen√ßa √© que o label √© deslocado em um token.\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // MAX_LENGTH) * MAX_LENGTH\n",
    "    result = {\n",
    "        k: [t[i : i + MAX_LENGTH] for i in range(0, total_length, MAX_LENGTH)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,  # Use num_proc > 1 se estiver em um ambiente com muitos cores de CPU\n",
    "    remove_columns=tokenized_datasets.column_names\n",
    ")\n",
    "\n",
    "# Divide o dataset (70% treino, 30% avalia√ß√£o)\n",
    "train_size = int(0.7 * len(lm_datasets))\n",
    "eval_size = len(lm_datasets) - train_size\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(lm_datasets, [train_size, eval_size])\n",
    "\n",
    "print(f\"Dados de Treino: {len(train_dataset)}, Dados de Avalia√ß√£o: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Configurar e Treinar o Modelo\n",
    "\n",
    "# Carrega o modelo base\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Argumentos de Treinamento (Ajuste conforme necess√°rio)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR, \n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                 # N√∫mero de passagens completas pelo dataset (epochs)\n",
    "    per_device_train_batch_size=2,      # Tamanho do lote de treinamento (diminua se a GPU ficar sem mem√≥ria)\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_steps=100,                     # Salva checkpoints a cada 100 passos\n",
    "    save_total_limit=2,                 # Limita o n√∫mero de checkpoints salvos\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available()  # Usa precis√£o mista (r√°pido e econ√¥mico na GPU)\n",
    ")\n",
    "\n",
    "# Inicializa o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator, \n",
    ")\n",
    "\n",
    "# Inicia o Fine-Tuning\n",
    "print(\"Iniciando Fine-Tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-Tuning conclu√≠do.\")\n",
    "\n",
    "# Salva o modelo e o tokenizer final\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Modelo personalizado Keisy salvo em: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Teste R√°pido do Modelo Treinado\n",
    "\n",
    "logging.info(\"Carregando e testando o modelo...\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Pergunta de teste\n",
    "prompt = \"Ol√°, quem √© voc√™?\"\n",
    "new_input_ids = loaded_tokenizer.encode(prompt + loaded_tokenizer.eos_token, return_tensors='pt').to(loaded_model.device)\n",
    "\n",
    "# Gera√ß√£o de resposta\n",
    "chat_history_ids = loaded_model.generate(\n",
    "    new_input_ids, \n",
    "    max_length=MAX_LENGTH,\n",
    "    do_sample=True, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    temperature=0.75,\n",
    "    pad_token_id=loaded_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = loaded_tokenizer.decode(chat_history_ids[:, new_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Resposta de Keisy (Personalizada): {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
